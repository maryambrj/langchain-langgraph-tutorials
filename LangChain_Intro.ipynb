{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbqLXtfPIzJR"
      },
      "source": [
        "# Introduction to LangChain and LangGraph\n",
        "\n",
        "Maryam Berijanian  \n",
        "_Last updated: 2025-10-06_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwbWpUjGJks2"
      },
      "source": [
        "# 1) Setup for Google Colab\n",
        "\n",
        "**Note: Google Colab Pro is free for students and teachers! Link [here.](https://colab.research.google.com/signup)**\n",
        "\n",
        "* We are using models from Hugging Face to avoid incurring costs associated with using paid APIs.\n",
        "\n",
        "* To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as a secret in your Google Colab (using the \"ðŸ”‘\" icon in the left panel) with the name `HF_TOKEN`, and restart your session. This secret can then be reused in all of your notebooks.\n",
        "\n",
        "* Install `transformers`, `accelerate`, `langchain`, `langgraph`, `langchain-huggingface`, `langchain-community`, `sentence-transformers`, and `faiss-cpu`.\n",
        "\n",
        "* Load a small chat model (**TinyLlama 1.1B Chat**). You can try `Qwen/Qwen2.5-1.5B-Instruct` or `Phi-3-mini-4k-instruct`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H09hNr-css1g"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U \\\n",
        "  \"requests==2.32.4\" \"numpy==2.0.2\" \\\n",
        "  \"transformers==4.41.2\" \"accelerate>=0.29,<1\" \\\n",
        "  \"langchain>=0.3,<0.4\" \"langchain-community>=0.3,<0.4\" \\\n",
        "  \"langgraph==0.2.*\" \"langchain-huggingface>=0.1,<0.4\" \\\n",
        "  \"sentence-transformers>=2.6,<3\"  \"wikipedia\" \\\n",
        "  \"sentencepiece\" \"faiss-cpu\" \"tiktoken\" \"ipywidgets\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eNVuXoZKcz3",
        "outputId": "ba9bff7f-ddc7-4d29-854c-9c519b39e307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: TinyLlama/TinyLlama-1.1B-Chat-v1.0 | device: GPU\n"
          ]
        }
      ],
      "source": [
        "import os, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "MODEL_NAME = os.environ.get(\"WORKSHOP_MODEL\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "mdl = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Common stop markers for chat-tuned HF models with role tokens\n",
        "STOP_TOKENS = [\"</s>\", \"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
        "\n",
        "gen_pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=mdl,\n",
        "    tokenizer=tok,\n",
        "    device=0 if use_gpu else -1,\n",
        "    do_sample=True,\n",
        "    temperature=0.3,\n",
        "    # max_new_tokens=256,\n",
        "    return_full_text=False,\n",
        "    eos_token_id=tok.eos_token_id,\n",
        "    pad_token_id=getattr(tok, \"pad_token_id\", tok.eos_token_id),\n",
        ")\n",
        "\n",
        "# Wrap in LangChain\n",
        "llm = HuggingFacePipeline(pipeline=gen_pipe)\n",
        "chat = ChatHuggingFace(llm=llm)\n",
        "\n",
        "print(\"Loaded:\", MODEL_NAME, \"| device:\", \"GPU\" if use_gpu else \"CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppw7xYEosfDW"
      },
      "source": [
        "# 2) Basic LLM Call  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yChyQmtaQrNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84cbf61-cd75-4ab1-cd2e-1bcefa415916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a 2-step plan to make tea:\n",
            "\n",
            "1. Boil water: Start by boiling water in a pot or kettle. You can use a microwave or a stovetop to heat the water.\n",
            "\n",
            "2. Add tea leaves: Once the water has boiled, add the tea leaves to the pot. You can use loose tea leaves or tea bags.\n",
            "\n",
            "3. Steep the tea: Allow the tea leaves to steep for the desired amount of time, usually 2-3 minutes.\n",
            "\n",
            "4. Remove the tea leaves: Once the tea has steeped, remove the tea leaves from the pot.\n",
            "\n",
            "5. Strain the tea: Pour the tea into a cup or mug and strain the tea leaves using a tea strainer or a cheesecloth.\n",
            "\n",
            "6. Enjoy: Serve the tea hot or cold, and enjoy the flavor and aroma of your homemade tea!\n"
          ]
        }
      ],
      "source": [
        "resp = chat.invoke(\n",
        "    [HumanMessage(content=\"Give me a 2-step plan to make tea.\")],\n",
        "    temperature=0.3,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        ")\n",
        "print(resp.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wy2Mdh4Z6G1"
      },
      "source": [
        "## LangChain\n",
        "\n",
        "LangChain is a framework designed to help developers build applications that leverage the power of large language models (LLMs).  \n",
        "It provides abstractions and components to easily create complex workflows, or \"chains,\" by combining different parts, such as prompts, models, and output parsers. Simple chaining involves connecting these components in a sequence, where the output of one component becomes the input for the next.\n",
        "\n",
        "## LangChain Expression Language (LCEL)\n",
        "\n",
        "LangChain Expression Language (LCEL) is a declarative way to compose chains. A key benefit of LCEL is its ability to facilitate the composition of different runnables (components that can be invoked).  \n",
        "This allows for the creation of sophisticated chains by simply piping components together using the `|` operator. LCEL makes it easy to build complex pipelines with features like streaming, async support, and parallel execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7CrmJXmaoKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee98250c-9b97-4e56-a87e-613cf0aa5a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A collaborative multi-agent system is a network of autonomous agents that work together to achieve a common goal. It involves the coordination and communication of multiple agents to achieve a shared objective, such as solving a complex problem or achieving a specific goal.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a concise assistant.\"),\n",
        "    (\"human\", \"In one sentence, explain what a collaborative multi-agent system is.\")\n",
        "])\n",
        "chain = prompt | chat | StrOutputParser()\n",
        "print(chain.invoke({}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "253ALmOGaAdt"
      },
      "source": [
        "# 3) Routing vs Chaining\n",
        "\n",
        "While basic chaining provides a fixed sequence of steps, more complex applications often require dynamic workflows where the next step depends on the output of the previous one or on some condition. This is where routing comes in.\n",
        "\n",
        "**Chaining** can be thought of as a linear path: `Step A` (Planner) -> `Step B` (Researcher) -> `Step C` (Writer).\n",
        "\n",
        "**Routing** allows for conditional paths: After `Step A` (Planner), based on the result, the workflow might go to `Step B` or `Step C` (skip Researcher for trivial tasks).\n",
        "\n",
        "LangGraph supports **conditional edges**, allowing you to implement routing logic and create workflows with loops and dynamic paths, which is essential for building sophisticated agents and coordinating multiple agents.\n",
        "\n",
        "**Example: Research Agent**\n",
        "\n",
        "Here is an example of a research agent; created once using a simple chain, and once using a graph with a conditional edge.  \n",
        "The result of both agents are the same, as they cannot actually look up the information on the internet!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from typing import TypedDict, Optional, Literal\n",
        "from langgraph.graph import StateGraph, END\n",
        "import re\n",
        "\n",
        "\n",
        "def first_n_lines(text: str, n: int) -> str:\n",
        "    lines = [ln.strip() for ln in (text or \"\").splitlines() if ln.strip()]\n",
        "    return \"\\n\".join(lines[:n])\n",
        "\n",
        "def first_line(text: str) -> str:\n",
        "    return (text or \"\").splitlines()[0].strip() if text else \"\""
      ],
      "metadata": {
        "id": "eKcThu4sHQzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_plan(raw: str, task: str) -> str:\n",
        "    \"\"\"Ensure 1â€“2 imperative lines, never echoing the task, numbered 1./2.\"\"\"\n",
        "    lines = [ln.strip() for ln in (raw or \"\").splitlines() if ln.strip()]\n",
        "    flt = [ln for ln in lines if task.lower() not in ln.lower() and \"task\" not in ln.lower() and \"?\" not in ln]\n",
        "    flt = flt[:2]\n",
        "    cleaned = []\n",
        "    for i, ln in enumerate(flt, 1):\n",
        "        ln = re.sub(r\"^\\s*(\\d+\\.|[-*â€¢])\\s*\", \"\", ln)\n",
        "        ln = ln.strip()\n",
        "        cleaned.append(f\"{i}. {ln}\")\n",
        "    if not cleaned or any(not re.search(r\"^\\d\\.\\s+\\w+\", ln) for ln in cleaned):\n",
        "        cleaned = [\"1. Search reliable sources\", \"2. Extract final answer\"]\n",
        "    return \"\\n\".join(cleaned[:2])\n",
        "\n",
        "def normalize_facts(raw: str) -> str:\n",
        "    \"\"\"Return 1â€“2 short sentences or 'Unknown'.\"\"\"\n",
        "    if not raw:\n",
        "        return \"Unknown\"\n",
        "    text = \" \".join(\n",
        "        re.sub(r\"^\\s*(\\d+\\.|[-*â€¢])\\s*\", \"\", ln.strip())\n",
        "        for ln in raw.splitlines()\n",
        "        if ln.strip()\n",
        "    )\n",
        "    if text.strip().lower() == \"unknown\":\n",
        "        return \"Unknown\"\n",
        "    parts = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    sents = [s.strip() for s in parts if s.strip() and s.strip().lower() != \"unknown\"]\n",
        "    if not sents:\n",
        "        return \"Unknown\"\n",
        "    # Keep at most 2 sentences, trim any overlong ones, and ensure terminal punctuation.\n",
        "    out = []\n",
        "    for s in sents:\n",
        "        words = s.split()\n",
        "        if len(words) > 24:  # keep sentences tight\n",
        "            s = \" \".join(words[:24]).rstrip(\",;:\") + \".\"\n",
        "        if not re.search(r'[.!?]$', s):\n",
        "            s += \".\"\n",
        "        out.append(s)\n",
        "        if len(out) == 2:\n",
        "            break\n",
        "    return \"\\n\".join(out) if out else \"Unknown\"\n",
        "    # If the model wrote sentences/verbs, keep short nouny spans; else, accept.\n",
        "    nouny = []\n",
        "    for ln in lines:\n",
        "        # cut after ~15 tokens and strip trailing punctuation/verbs cues\n",
        "        ln = re.sub(r\"\\b(identify|verify|determine|explain|check|search|look up)\\b.*\", \"\", ln, flags=re.I).strip()\n",
        "        ln = re.sub(r\"[.;:]\\s*$\", \"\", ln)\n",
        "        tokens = ln.split()\n",
        "        ln = \" \".join(tokens[:15]).strip()\n",
        "        if ln:\n",
        "            nouny.append(ln)\n",
        "    return \"\\n\".join(nouny[:2]) if nouny else \"Unknown\"\n",
        "\n",
        "def normalize_answer(raw: str, facts: str) -> str:\n",
        "    \"\"\"Single short phrase (<=15 words), exactly 'unknown' if facts unknown/insufficient.\"\"\"\n",
        "    if not facts or facts.strip().lower() == \"unknown\":\n",
        "        return \"unknown\"\n",
        "    out = first_line(raw)\n",
        "    out = re.sub(r\"^\\s*(output|answer|final|result)\\s*[:\\-]\\s*\", \"\", out, flags=re.I)\n",
        "    out = out.strip().strip('\"').strip(\"'\")\n",
        "    out = \" \".join(out.split())\n",
        "    if not out or out.lower() in {\"unknown\", \"n/a\", \"not sure\"}:\n",
        "        out = first_line(facts)\n",
        "    out = \" \".join(out.split()[:100]).strip()\n",
        "    return out if out else \"unknown\""
      ],
      "metadata": {
        "id": "LGVh859HHQ7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "    task: str\n",
        "    plan: Optional[str]\n",
        "    facts: Optional[str]\n",
        "    draft: Optional[str]\n",
        "    label: Optional[str]\n",
        "\n",
        "# Prompts\n",
        "PlanP = ChatPromptTemplate.from_template(\n",
        "    \"Return ONLY 1â€“2 numbered imperative steps.\\n\"\n",
        "    \"Do NOT restate or quote the TASK. No questions.\\n\\n\"\n",
        "    \"TASK:\\n{task}\\n\\n\"\n",
        "    \"1.\"\n",
        ")\n",
        "\n",
        "FactsP = ChatPromptTemplate.from_template(\n",
        "    \"Return ONLY 1â€“2 short sentences with facts needed to answer the TASK.\\n\"\n",
        "    \"If unknown, write exactly: Unknown\\n\\n\"\n",
        "    \"TASK:\\n{task}\\n\"\n",
        "    \"PLAN:\\n{plan}\\n\\n\"\n",
        "    \"-\"\n",
        ")\n",
        "\n",
        "WriteP = ChatPromptTemplate.from_template(\n",
        "    \"Output ONLY the final answer as a short phrase (not more than 100 words).\\n\"\n",
        "    \"If facts are Unknown or insufficient, output exactly: unknown\\n\"\n",
        "    \"No labels, no quotes, no extra words.\\n\\n\"\n",
        "    \"TASK:\\n{task}\\n\"\n",
        "    \"FACTS:\\n{facts}\\n\\n\"\n",
        ")\n",
        "\n",
        "ClassifyP = ChatPromptTemplate.from_template(\n",
        "    \"Output exactly one token: trivial or research_needed.\\n\"\n",
        "    \"TASK:\\n{task}\\n\"\n",
        "    \"Label:\"\n",
        ")"
      ],
      "metadata": {
        "id": "mL52GTX3HQ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nodes\n",
        "def plan_node(s: State):\n",
        "    raw = (PlanP | chat | StrOutputParser()).invoke({\"task\": s[\"task\"]})\n",
        "    plan = normalize_plan(first_n_lines(raw, 2), s[\"task\"])\n",
        "    return {**s, \"plan\": plan}\n",
        "\n",
        "def research_node(s: State):\n",
        "    raw = (FactsP | chat | StrOutputParser()).invoke({\"task\": s[\"task\"], \"plan\": s.get(\"plan\") or \"\"})\n",
        "    facts = normalize_facts(first_n_lines(raw, 2))\n",
        "    return {**s, \"facts\": facts}\n",
        "\n",
        "def write_node(s: State):\n",
        "    facts = s.get(\"facts\") or \"Unknown\"\n",
        "    if facts.strip().lower() == \"unknown\":\n",
        "        return {**s, \"draft\": \"unknown\"}\n",
        "    raw = (WriteP | chat | StrOutputParser()).invoke({\"task\": s[\"task\"], \"facts\": facts})\n",
        "    ans = normalize_answer(raw, facts)\n",
        "    return {**s, \"draft\": ans}\n",
        "\n",
        "def classify_node(s: State):\n",
        "    lab = (ClassifyP | chat | StrOutputParser()).invoke({\"task\": s[\"task\"]}).strip().lower()\n",
        "    lab = \"trivial\" if lab.startswith(\"trivial\") else \"research_needed\"\n",
        "    return {**s, \"label\": lab}\n",
        "\n",
        "def route_after_plan(s: State) -> Literal[\"skip\",\"do_research\"]:\n",
        "    return \"skip\" if (s.get(\"label\") or \"research_needed\") == \"trivial\" else \"do_research\""
      ],
      "metadata": {
        "id": "i813GNbeJaSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (planning_node -> research -> write) and the routed graph with classify/conditional edge.\n",
        "\n",
        "# Pure chain\n",
        "g_chain = StateGraph(State)\n",
        "g_chain.add_node(\"planning_node\", plan_node)\n",
        "g_chain.add_node(\"research\", research_node)\n",
        "g_chain.add_node(\"write\", write_node)\n",
        "g_chain.set_entry_point(\"planning_node\")\n",
        "g_chain.add_edge(\"planning_node\", \"research\")\n",
        "g_chain.add_edge(\"research\", \"write\")\n",
        "g_chain.add_edge(\"write\", END)\n",
        "app_chain = g_chain.compile()"
      ],
      "metadata": {
        "id": "unfgUR3lJiPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Routed graph\n",
        "g_route = StateGraph(State)\n",
        "g_route.add_node(\"classify\", classify_node)\n",
        "g_route.add_node(\"planning_node\", plan_node)\n",
        "g_route.add_node(\"research\", research_node)\n",
        "g_route.add_node(\"write\", write_node)\n",
        "g_route.set_entry_point(\"classify\")\n",
        "g_route.add_edge(\"classify\", \"planning_node\")\n",
        "g_route.add_conditional_edges(\"planning_node\", route_after_plan, {\"skip\": \"write\", \"do_research\": \"research\"})\n",
        "g_route.add_edge(\"research\", \"write\")\n",
        "g_route.add_edge(\"write\", END)\n",
        "app_route = g_route.compile()"
      ],
      "metadata": {
        "id": "dNTyl5XCGQ91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaRYBPm2pCIv",
        "outputId": "f64e0f94-db75-4b1e-e347-2ad2b097d583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CHAIN ===\n",
            "PLAN:\n",
            " 1. Search reliable sources\n",
            "2. Extract final answer\n",
            "FACTS:\n",
            " Sure, here's a plan for answering the TASK: Search reliable sources:.\n",
            "DRAFT:\n",
            " Here's the final answer as a short phrase (not more than 100 words) for the TASK: \n",
            "\n",
            "=== ROUTE ===\n",
            "LABEL: research_needed\n",
            "PLAN:\n",
            " 1. Answer: The current fashion trends are trendy and popular among people.\n",
            "FACTS:\n",
            " TASK: What is the current fashion trends in the fashion industry?\n",
            "DRAFT:\n",
            " TASK: What is the current fashion trends in the fashion industry?\n"
          ]
        }
      ],
      "source": [
        "q = {\"task\": \"What is the current fashion trends?\"}\n",
        "\n",
        "s1 = app_chain.invoke(q)\n",
        "print(\"=== CHAIN ===\")\n",
        "print(\"PLAN:\\n\", s1.get(\"plan\"))\n",
        "print(\"FACTS:\\n\", s1.get(\"facts\"))\n",
        "print(\"DRAFT:\\n\", s1.get(\"draft\"), \"\\n\")\n",
        "\n",
        "s2 = app_route.invoke(q)\n",
        "print(\"=== ROUTE ===\")\n",
        "print(\"LABEL:\", s2.get(\"label\"))\n",
        "print(\"PLAN:\\n\", s2.get(\"plan\"))\n",
        "print(\"FACTS:\\n\", s2.get(\"facts\"))\n",
        "print(\"DRAFT:\\n\", s2.get(\"draft\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azxZ0g8oSJTd"
      },
      "source": [
        "# 4) Tool Use\n",
        "\n",
        "[Tools](https://python.langchain.com/docs/concepts/tools/) are external functions or services that agents can [call](https://python.langchain.com/docs/concepts/tool_calling/) (e.g., calculators, search APIs, databases). They extend LLMs beyond text generation, enabling actions like calculations, fetching real-time info, or API integration.\n",
        "\n",
        "### Creating Tools\n",
        "\n",
        "Use the `@tool` decorator to turn Python functions into LangChain-compatible tools. The decorator infers name, description, and arguments automatically.\n",
        "\n",
        "[Model Context Protocol (MCP) servers](https://github.com/langchain-ai/langchain-mcp-adapters) can also serve as tools.\n",
        "\n",
        "### How They Work\n",
        "\n",
        "Tools are made available to models through binding. With function-callingâ€“capable models, tools can be bound directly using `bind_tools`.\n",
        "\n",
        "When a tool is bound to the model, the model can choose to call the tool by returning a structured output with tool arguments. We use the `bind_tools` method to augment an LLM with tools.\n",
        "\n",
        "For other models, agents (like `AgentExecutor` or LangGraph-based agents) handle tool invocation logic.\n",
        "\n",
        "### Tool Invocation\n",
        "\n",
        "LLMs can call tools by returning structured outputs with arguments. You can control tool use:\n",
        "\n",
        "  * [`tool_choice`](https://python.langchain.com/docs/how_to/tool_choice/) parameter â†’ enforce or restrict tool calls (`any`, specific, etc.).\n",
        "  * [`parallel_tool_calls=False`](https://python.langchain.com/docs/how_to/tool_calling_parallel/) â†’ limit to one tool call at a time.\n",
        "\n",
        "\n",
        "### Demo\n",
        "\n",
        "For this basic demonstration, we'll simulate a simple flow where the model *could* use the tool if it understood. The current setup with a simple chain might not automatically invoke the tool.\n",
        "\n",
        "A full agent framework (like LangGraph agents) is needed for the model to dynamically decide to call the tool and process its output.\n",
        "\n",
        "Although this direct binding might not trigger tool use with all models/setups in a simple chain, it conceptually shows how tools are made available to the model within LangChain runnables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEsqVzTtDdby"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import datetime as dt\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.agents import tool\n",
        "\n",
        "\n",
        "class OpenMeteoInput(BaseModel):\n",
        "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
        "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
        "\n",
        "\n",
        "@tool(args_schema=OpenMeteoInput)\n",
        "def get_current_temperature(latitude: float, longitude: float) -> str:\n",
        "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
        "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "    params = {\n",
        "        \"latitude\": latitude,\n",
        "        \"longitude\": longitude,\n",
        "        \"hourly\": \"temperature_2m\",\n",
        "        \"forecast_days\": 1,\n",
        "        \"timezone\": \"UTC\",\n",
        "    }\n",
        "\n",
        "    def _parse_utc(ts: str) -> dt.datetime:\n",
        "      if ts.endswith(\"Z\"):\n",
        "          return dt.datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
        "      d = dt.datetime.fromisoformat(ts)\n",
        "      return d.replace(tzinfo=dt.timezone.utc) if d.tzinfo is None else d.astimezone(dt.timezone.utc)\n",
        "\n",
        "    r = requests.get(BASE_URL, params=params, timeout=20)\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"API request failed: {r.status_code} {r.text[:200]}\")\n",
        "    data = r.json()\n",
        "\n",
        "    now_utc = dt.datetime.now(dt.timezone.utc)\n",
        "    times = [_parse_utc(t) for t in data[\"hourly\"][\"time\"]]\n",
        "    temps = data[\"hourly\"][\"temperature_2m\"]\n",
        "\n",
        "    idx = min(range(len(times)), key=lambda i: abs(times[i] - now_utc))\n",
        "    temp_c = temps[idx]\n",
        "    when = times[idx].isoformat()\n",
        "\n",
        "    return f\"The current temperature near ({latitude:.3f}, {longitude:.3f}) is {temp_c}Â°C (closest hour: {when}).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nzjnaF4Drta",
        "outputId": "b8494131-3794-4a1d-fd87-705757817992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'get_current_temperature', 'description': 'Fetch current temperature for given coordinates.', 'parameters': {'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for', 'type': 'number'}, 'longitude': {'description': 'Longitude of the location to fetch weather data for', 'type': 'number'}}, 'required': ['latitude', 'longitude'], 'type': 'object'}}\n",
            "The current temperature near (13.000, 14.000) is 24.3Â°C (closest hour: 2025-10-07T04:00:00+00:00).\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "print(convert_to_openai_function(get_current_temperature))\n",
        "print(get_current_temperature.invoke({\"latitude\": 13, \"longitude\": 14}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTx3A85tDdib",
        "outputId": "21c31ce7-2820-4e2a-e312-b8fa3eb4e84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'search_wikipedia', 'description': 'Run Wikipedia search and get page summaries.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}\n",
            "Page: LangChain\n",
            "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
            "\n",
            "Page: Vector database\n",
            "Summary: A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more approximate nearest neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\n",
            "Vectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector's position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\n",
            "These feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\n",
            "Vector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\n",
            "Vector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\n",
            "\n",
            "Page: Retrieval-augmented generation\n",
            "Summary: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\n",
            "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\n",
            "RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\n",
            "The term RAG was first introduced in a 2020 research paper from Meta.\n"
          ]
        }
      ],
      "source": [
        "import wikipedia\n",
        "\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
        "    titles = wikipedia.search(query) or []\n",
        "    summaries = []\n",
        "    for title in titles[:3]:\n",
        "        try:\n",
        "            page = wikipedia.page(title=title, auto_suggest=False)\n",
        "            summaries.append(f\"Page: {title}\\nSummary: {page.summary}\")\n",
        "        except (wikipedia.exceptions.PageError, wikipedia.exceptions.DisambiguationError):\n",
        "            continue\n",
        "    return \"\\n\\n\".join(summaries) if summaries else \"No good Wikipedia Search Result was found\"\n",
        "\n",
        "print(convert_to_openai_function(search_wikipedia))\n",
        "print(search_wikipedia.invoke(\"langchain\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UcUUH1gmEiv"
      },
      "source": [
        "# 5) Reflection\n",
        "\n",
        "LLM agents can improve their outputs through reflection, a process where they evaluate and refine their work. There are two main types of reflection:\n",
        "\n",
        "1.  **Self-reflection**: The agent critiques its *own* generated output. It acts as both the generator and the critic, identifying potential issues and suggesting revisions based on internal criteria or prompts. This allows the agent to iteratively improve its response without external input.\n",
        "\n",
        "2.  **External reflection**: A *separate* agent or component acts as a critic, providing feedback on the initial draft generated by the primary agent. This external feedback can offer a different perspective and potentially more targeted or structured suggestions for revision, leading to a refined final output.\n",
        "\n",
        "By incorporating reflection, agents can produce higher-quality, more accurate, and better-structured responses, especially for complex tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSc3q4CxElDR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "Writer = ChatPromptTemplate.from_template(\n",
        "    \"Write a sentence about: {topic}. \"\n",
        ")\n",
        "\n",
        "SelfCrit = ChatPromptTemplate.from_template(\n",
        "    \"You wrote:\\n{draft}\\n\\n\"\n",
        "    \"Identify a word that could be expressed in a simpler way. Return exactly ONE word. \"\n",
        ")\n",
        "\n",
        "ExtCrit = ChatPromptTemplate.from_template(\n",
        "    \"Identify a word that could be expressed in a simpler way. Return exactly ONE word. Word:\\n{draft}\\n\\n\"\n",
        ")\n",
        "\n",
        "Reviser = ChatPromptTemplate.from_template(\n",
        "    \"Revise the draft using the issue below. Return ONLY the revised sentences. \\n\\n\"\n",
        "    \"Draft:\\n{draft}\\n\\nIssue:\\n{fb}\\nRevised text:\\n\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE_hjFaLEodL"
      },
      "outputs": [],
      "source": [
        "def self_reflect(topic: str):\n",
        "    \"\"\"Generates a draft, self-critiques it, and revises.\"\"\"\n",
        "    print(f\"--- Self Reflection on: {topic} ---\")\n",
        "    print(\"Generating initial draft...\")\n",
        "    draft = (Writer | chat | StrOutputParser()).invoke({\"topic\": topic})\n",
        "    print(\"Initial Draft generated.\")\n",
        "\n",
        "    print(\"Critiquing own draft...\")\n",
        "    feedback = (SelfCrit | chat | StrOutputParser()).invoke({\"draft\": draft})\n",
        "    print(\"Self-critique generated.\")\n",
        "\n",
        "    print(\"Revising based on self-critique...\")\n",
        "    revised = (Reviser | chat | StrOutputParser()).invoke({\"draft\": draft, \"fb\": feedback})\n",
        "    print(\"Revised draft generated.\")\n",
        "    return draft, feedback, revised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GubcWAKSE8tT",
        "outputId": "6b1176ba-a28f-458d-aacb-63f76c5d7a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Self Reflection on: LangChain ---\n",
            "Generating initial draft...\n",
            "Initial Draft generated.\n",
            "Critiquing own draft...\n",
            "Self-critique generated.\n",
            "Revising based on self-critique...\n",
            "Revised draft generated.\n",
            "\n",
            "--- Self Reflection Results ---\n",
            "Original Draft:\n",
            " LangChain is a platform that provides a seamless and efficient way to translate text, documents, and multimedia content from one language to another.\n",
            "---------------------\n",
            "\n",
            "Self Feedback:\n",
            " \"A seamless and efficient way\" can be expressed in a simpler way as \"a way that is seamless and efficient.\"\n",
            "---------------------\n",
            "\n",
            "Revised Draft:\n",
            " LangChain is a platform that provides a user-friendly and scalable solution to translate text, documents, and multimedia content from one language to another.\n",
            "\n",
            "Issue:\n",
            "\"A seamless and efficient way\" can be expressed in a simpler way as \"a way that is efficient and seamless.\"\n"
          ]
        }
      ],
      "source": [
        "topic = \"LangChain\"\n",
        "d0, fb0, r0 = self_reflect(topic)\n",
        "\n",
        "print(\"\\n--- Self Reflection Results ---\")\n",
        "print(\"Original Draft:\\n\", d0)\n",
        "print(\"---------------------\")\n",
        "print(\"\\nSelf Feedback:\\n\", fb0)\n",
        "print(\"---------------------\")\n",
        "print(\"\\nRevised Draft:\\n\", r0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3di64DdRE145"
      },
      "outputs": [],
      "source": [
        "def external_reflect(topic: str):\n",
        "    \"\"\"Generates a draft, gets external critique, and revises.\"\"\"\n",
        "    print(f\"\\n--- External Reflection on: {topic} ---\")\n",
        "    print(\"Generating initial draft...\")\n",
        "    draft = (Writer | chat | StrOutputParser()).invoke({\"topic\": topic})\n",
        "    print(\"Initial Draft generated.\")\n",
        "\n",
        "    print(\"Getting external critique...\")\n",
        "    feedback = (ExtCrit | chat | StrOutputParser()).invoke({\"draft\": draft})\n",
        "    print(\"External critique generated.\")\n",
        "\n",
        "    print(\"Revising based on external critique...\")\n",
        "    revised = (Reviser | chat | StrOutputParser()).invoke({\"draft\": draft, \"fb\": feedback})\n",
        "    print(\"Revised draft generated.\")\n",
        "    return draft, feedback, revised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "380072ae",
        "outputId": "8d487713-99f8-4101-fe49-9c7dde43b4d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- External Reflection on: LangChain ---\n",
            "Generating initial draft...\n",
            "Initial Draft generated.\n",
            "Getting external critique...\n",
            "External critique generated.\n",
            "Revising based on external critique...\n",
            "Revised draft generated.\n",
            "\n",
            "--- External Reflection Results ---\n",
            "Original Draft:\n",
            " LangChain is a software solution that enables businesses to translate their website and online content into multiple languages.\n",
            "---------------------\n",
            "\n",
            "External Feedback:\n",
            " Simplify the word \"LangChain\" into a single word: \"LangCh.\"\n",
            "---------------------\n",
            "\n",
            "Revised Draft:\n",
            " Draft:\n",
            "LangCh is a software solution that enables businesses to translate their website and online content into multiple languages.\n",
            "\n",
            "Issue:\n",
            "Simplify the word \"LangCh\" into a single word: \"LangCh.\"\n"
          ]
        }
      ],
      "source": [
        "d1, fb1, r1 = external_reflect(topic)\n",
        "\n",
        "print(\"\\n--- External Reflection Results ---\")\n",
        "print(\"Original Draft:\\n\", d1)\n",
        "print(\"---------------------\")\n",
        "print(\"\\nExternal Feedback:\\n\", fb1)\n",
        "print(\"---------------------\")\n",
        "print(\"\\nRevised Draft:\\n\", r1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e0P5Qe0m9Ug"
      },
      "source": [
        "# 6) Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique that combines the power of LLMs with external knowledge sources to generate more accurate and informed responses.\n",
        "\n",
        "Instead of relying solely on the knowledge encoded in the model's parameters during training, RAG systems retrieve relevant information from a separate knowledge base (like a database, a collection of documents, or the internet) and use this retrieved information as context for the language model when generating an answer.\n",
        "\n",
        "This approach helps to reduce the likelihood of the model hallucinating or providing outdated information, making the generated responses more reliable and grounded in factual data.\n",
        "\n",
        "In this section, we will build a simple in-memory RAG system using:\n",
        "- **FAISS**: A library for efficient similarity search and clustering of dense vectors. We'll use it as our vector store to index and search our documents.\n",
        "- **HuggingFaceEmbeddings**: A component from `langchain-huggingface` to generate vector representations (embeddings) of our text documents and queries using models from the Hugging Face Hub.\n",
        "- **LangChain**: To orchestrate the process of retrieving documents and feeding them into a prompt for the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEJr4vf9FGwG",
        "outputId": "cfa19865-b9af-4a80-e3fb-f58ca825d9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating embeddings and vector store...\n",
            "Vector store created successfully.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    category=FutureWarning,\n",
        "    message=\"`resume_download` is deprecated\"\n",
        ")\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"LangGraph enables stateful, multi-agent workflows represented as graphs with nodes and edges.\"),\n",
        "    Document(page_content=\"LCEL, or LangChain Expression Language, provides a declarative way to compose LangChain components like prompts, models, and parsers using the pipe operator (|).\"),\n",
        "    Document(page_content=\"Agents in LangChain can call external tools, such as search APIs or calculators, to perform actions and access up-to-date information.\"),\n",
        "    Document(page_content=\"Reflection techniques, both self and external, can be used to improve the quality of agent-generated outputs through critique and revision loops.\"),\n",
        "    Document(page_content=\"RAG, or Retrieval-Augmented Generation, enhances LLMs by retrieving relevant information from external knowledge bases to ground the model's responses.\"),\n",
        "    Document(page_content=\"FAISS is a library for efficient similarity search on large sets of vectors. It is commonly used as a vector store in RAG systems.\"),\n",
        "    Document(page_content=\"HuggingFaceEmbeddings allows generating embeddings using various transformer models available on the Hugging Face Hub.\")\n",
        "]\n",
        "\n",
        "print(\"Creating embeddings and vector store...\")\n",
        "try:\n",
        "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vs = FAISS.from_documents(docs, emb)\n",
        "    print(\"Vector store created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating embeddings or vector store: {e}\")\n",
        "    vs = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJNJGS_GFLzB"
      },
      "outputs": [],
      "source": [
        "def rag_answer(q: str, k: int = 3):\n",
        "    \"\"\"\n",
        "    Performs RAG to answer a question using the FAISS vector store.\n",
        "\n",
        "    Args:\n",
        "        q: The question to answer.\n",
        "        k: The number of documents to retrieve (default is 3).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the retrieved documents and the generated answer,\n",
        "        or None, None if the vector store is not available.\n",
        "    \"\"\"\n",
        "    if vs is None:\n",
        "        print(\"Vector store not initialized. Cannot answer question.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\nAnswering question: '{q}'\")\n",
        "    print(f\"Retrieving top {k} documents...\")\n",
        "\n",
        "    hits = vs.similarity_search(q, k=k)\n",
        "    print(f\"Retrieved {len(hits)} documents.\")\n",
        "\n",
        "    context = \"\\n\\n\".join(f\"[{i+1}] {d.page_content}\" for i, d in enumerate(hits))\n",
        "    print(\"Formatted context.\")\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Use the following context to answer the question succinctly. If the context does not contain the answer, state that you cannot answer based on the provided information.\\n\\nContext:\\n{ctx}\\n\\nQuestion: {q}\"\n",
        "    )\n",
        "    print(\"Prompt template created.\")\n",
        "\n",
        "    rag_chain = prompt | chat | StrOutputParser()\n",
        "    print(\"RAG chain created.\")\n",
        "\n",
        "    print(\"Invoking RAG chain...\")\n",
        "    ans = rag_chain.invoke({\"ctx\": context, \"q\": q})\n",
        "    print(\"RAG chain invoked.\")\n",
        "\n",
        "    return hits, ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4594baa4",
        "outputId": "87f44d10-6630-42f0-ebb0-fd792b744200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answering question: 'What are some key concepts in LangChain for building agentic systems?'\n",
            "Retrieving top 3 documents...\n",
            "Retrieved 3 documents.\n",
            "Formatted context.\n",
            "Prompt template created.\n",
            "RAG chain created.\n",
            "Invoking RAG chain...\n",
            "RAG chain invoked.\n",
            "\n",
            "--- RAG Results ---\n",
            "Context documents retrieved:\n",
            "1. Agents in LangChain can call external tools, such as search APIs or calculators, to perform actions ...\n",
            "2. LangGraph enables stateful, multi-agent workflows represented as graphs with nodes and edges....\n",
            "3. LCEL, or LangChain Expression Language, provides a declarative way to compose LangChain components l...\n",
            "\n",
            "Generated Answer:\n",
            "The following key concepts in LangChain for building agentic systems are:\n",
            "\n",
            "1. Agents: LangChain agents are the building blocks of agentic systems. They represent the behavior and actions of individual agents in a system.\n",
            "\n",
            "2. Context: The context is the environment in which an agent operates. It includes information about the environment, the agents, and the system as a whole.\n",
            "\n",
            "3. External tools: LangChain allows agents to call external tools, such as search APIs or calculators, to perform actions and access up-to-date information.\n",
            "\n",
            "4. Graphs: LangGraph enables stateful, multi-agent workflows represented as graphs with nodes and edges.\n",
            "\n",
            "5. LCEL: LangChain Expression Language provides a declarative way to compose LangChain components like prompts, models, and parsers using the pipe operator (|).\n",
            "\n",
            "6. Declarative: LangChain is declarative, which means that it defines the behavior of an agent in a system rather than specifying the details of how the system operates.\n",
            "\n",
            "7. Stateful: LangChain allows agents to maintain state, which is a critical aspect of agentic systems.\n",
            "\n",
            "8. Multi-agent: LangChain enables the creation of multi-agent systems, where multiple agents work together to achieve a common goal.\n",
            "\n",
            "9. Declarative programming: LangChain is a declarative programming language, which means that it provides a clear and concise way to express the behavior of an agent in a system.\n"
          ]
        }
      ],
      "source": [
        "question_rag = \"What are some key concepts in LangChain for building agentic systems?\"\n",
        "retrieved_docs, answer = rag_answer(question_rag)\n",
        "\n",
        "if retrieved_docs is not None and answer is not None:\n",
        "    print(\"\\n--- RAG Results ---\")\n",
        "    print(\"Context documents retrieved:\")\n",
        "    for i, d in enumerate(retrieved_docs, 1):\n",
        "        print(f\"{i}. {d.page_content[:100]}...\")\n",
        "    print(\"\\nGenerated Answer:\")\n",
        "    print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0UTjMbHFSr2",
        "outputId": "7feb4220-8353-46a6-b8ef-0e144e503793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answering question: 'What is the boiling point of water?'\n",
            "Retrieving top 1 documents...\n",
            "Retrieved 1 documents.\n",
            "Formatted context.\n",
            "Prompt template created.\n",
            "RAG chain created.\n",
            "Invoking RAG chain...\n",
            "RAG chain invoked.\n",
            "\n",
            "--- RAG Results (Unrelated Question) ---\n",
            "Context documents retrieved:\n",
            "1. LCEL, or LangChain Expression Language, provides a declarative way to compose LangChain components l...\n",
            "\n",
            "Generated Answer:\n",
            "Based on the given context, the boiling point of water is not explicitly mentioned. Therefore, it is not possible to answer the question based on the provided information.\n"
          ]
        }
      ],
      "source": [
        "question_unrelated = \"What is the boiling point of water?\"\n",
        "retrieved_docs_unrelated, answer_unrelated = rag_answer(question_unrelated,1)\n",
        "\n",
        "if retrieved_docs_unrelated is not None and answer_unrelated is not None:\n",
        "    print(\"\\n--- RAG Results (Unrelated Question) ---\")\n",
        "    print(\"Context documents retrieved:\")\n",
        "    for i, d in enumerate(retrieved_docs_unrelated, 1):\n",
        "        print(f\"{i}. {d.page_content[:100]}...\")\n",
        "    print(\"\\nGenerated Answer:\")\n",
        "    print(answer_unrelated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty6LsIqn4erY"
      },
      "source": [
        "# 7) Multi-Agent Collaboration\n",
        "\n",
        "The Multi-Agent Collaboration pattern involves designing systems where multiple independent or semi-independent agents work together to achieve a common goal. Each agent typically has a defined role, specific goals aligned with the overall objective, and potentially access to different tools or knowledge bases. The power of this pattern lies in the interaction and synergy between these agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKLGzt754hkh"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define agent roles\n",
        "def researcher_task(topic):\n",
        "  prompt = f\"You are a Researcher. Gather 3 factual bullet points about: {topic}\"\n",
        "  return llm.invoke(prompt)\n",
        "\n",
        "def analyst_task(notes):\n",
        "  prompt = f\"You are an Analyst. Given these research notes: \\n{notes}\\nSummarize the key insight in one sentence.\"\n",
        "  return llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLlS4lviQ8xO",
        "outputId": "74906459-9509-4470-ee11-34cf4b988104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Researcher Notes --\n",
            " , his work on the Enigma machine, and his involvement in the Dunkirk evacuation. Use a formal tone and provide specific examples to support your claims.\n",
            "\n",
            "--- Analyst Summary ---\n",
            " \n",
            "\"The Enigma machine played a critical role in the Dunkirk evacuation, allowing Allied forces to communicate with each other and coordinate their efforts.\" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Run collaboration\n",
        "topic = \"Alan Turing's contributions to computer science\"\n",
        "notes = researcher_task(topic)\n",
        "final = analyst_task(notes)\n",
        "# Step 3: Output\n",
        "print(\"\\n--- Researcher Notes --\\n\", notes)\n",
        "print(\"\\n--- Analyst Summary ---\\n\", final, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL_kUbWg4iHj"
      },
      "source": [
        "# 8) Human in the Loop\n",
        "\n",
        "The Human-in-the-Loop (HITL) pattern integrates AI with human input to enhance Agent capabilities. This approach acknowledges that optimal AI performance frequently requires a combination of automated processing and human insight, especially in scenarios with high complexity or ethical considerations. Rather than replacing human input, HITL aims to augment human abilities by ensuring that critical judgments and decisions are informed by human understanding.\n",
        "\n",
        "Despite its benefits, the HITL pattern has significant caveats, chief among them being a lack of scalability. While human oversight provides high accuracy, operators cannot manage millions of tasks, creating a fundamental trade-off that often requires a hybrid approach combining automation for scale and HITL for accuracy. Furthermore, the effectiveness of this pattern is heavily dependent on the expertise of the human operators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf2R55reuI8X"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "from uuid import uuid4\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import (\n",
        "    AnyMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        ")\n",
        "\n",
        "# toy tool\n",
        "@tool\n",
        "def search(query: str) -> str:\n",
        "  \"\"\"A dummy search tool that returns a temperature.\"\"\"\n",
        "  return \"42f\"\n",
        "\n",
        "# 1) Gather the tools from earlier\n",
        "TOOLS = []\n",
        "try:\n",
        "    TOOLS.append(search)\n",
        "except NameError:\n",
        "    pass\n",
        "try:\n",
        "    TOOLS.append(search_wikipedia)\n",
        "except NameError:\n",
        "    pass\n",
        "try:\n",
        "    TOOLS.append(get_current_temperature)\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "\n",
        "CITY_TO_COORDS = {\n",
        "    \"san francisco\": (37.7749, -122.4194),\n",
        "    \"sf\": (37.7749, -122.4194),\n",
        "    \"los angeles\": (34.0522, -118.2437),\n",
        "    \"la\": (34.0522, -118.2437),\n",
        "    \"new york\": (40.7128, -74.0060),\n",
        "    \"nyc\": (40.7128, -74.0060),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC69ujDSuJCU"
      },
      "outputs": [],
      "source": [
        "# 2) Message reducer\n",
        "def reduce_messages(left: list[AnyMessage], right: list[AnyMessage]) -> list[AnyMessage]:\n",
        "    for message in right:\n",
        "        if not getattr(message, \"id\", None):\n",
        "            message.id = str(uuid4())\n",
        "    merged = left.copy()\n",
        "    for message in right:\n",
        "        for i, existing in enumerate(merged):\n",
        "            if existing.id == message.id:\n",
        "                merged[i] = message\n",
        "                break\n",
        "        else:\n",
        "            merged.append(message)\n",
        "    return merged\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], reduce_messages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNEeJidmuamg"
      },
      "outputs": [],
      "source": [
        "# 3) Simple heuristic router deciding which tool to call (if any)\n",
        "def pick_tool_and_args(user_text: str):\n",
        "    text = (user_text or \"\").lower().strip()\n",
        "\n",
        "    # Weather â†’ prefer get_current_temperature if available\n",
        "    if any(k in text for k in (\"weather\", \"temperature\", \"temp\")):\n",
        "        if \"get_current_temperature\" in {t.name for t in TOOLS}:\n",
        "            # find a city in text\n",
        "            for key, (lat, lon) in CITY_TO_COORDS.items():\n",
        "                if key in text:\n",
        "                    return \"get_current_temperature\", {\"latitude\": lat, \"longitude\": lon}\n",
        "            # If no city detected, fall back to SF for demo\n",
        "            lat, lon = CITY_TO_COORDS[\"san francisco\"]\n",
        "            return \"get_current_temperature\", {\"latitude\": lat, \"longitude\": lon}\n",
        "\n",
        "    # Wikipedia-ish queries\n",
        "    if any(k in text for k in (\"wiki\", \"wikipedia\", \"who is\", \"what is\", \"tell me about\")):\n",
        "        if \"search_wikipedia\" in {t.name for t in TOOLS}:\n",
        "            return \"search_wikipedia\", user_text\n",
        "\n",
        "    # Generic search fallback (toy tool)\n",
        "    if \"search\" in {t.name for t in TOOLS} and any(k in text for k in (\"search\", \"look up\", \"find\")):\n",
        "        return \"search\", user_text\n",
        "\n",
        "    return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2cEVJqQuapF"
      },
      "outputs": [],
      "source": [
        "# 4) Agent with our Hugging Face chat model and router\n",
        "memory = MemorySaver()\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, model, tools, system: str = \"\", checkpointer=None):\n",
        "        self.model = model\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "        self.system = system\n",
        "\n",
        "        graph = StateGraph(AgentState)\n",
        "        graph.add_node(\"llm\", self.call_model)\n",
        "        graph.add_node(\"action\", self.take_action)\n",
        "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
        "        graph.add_edge(\"action\", \"llm\")\n",
        "        graph.set_entry_point(\"llm\")\n",
        "\n",
        "        self.graph = graph.compile(checkpointer=checkpointer, interrupt_before=[\"action\"])\n",
        "\n",
        "    def call_model(self, state: AgentState):\n",
        "      raw = state[\"messages\"]\n",
        "\n",
        "      msgs = [m for m in raw if isinstance(m, (SystemMessage, HumanMessage, AIMessage))]\n",
        "\n",
        "      if not msgs or not isinstance(msgs[-1], HumanMessage):\n",
        "          last_tool = next((m for m in reversed(raw) if isinstance(m, ToolMessage)), None)\n",
        "          last_human = next((m for m in reversed(raw) if isinstance(m, HumanMessage)), None)\n",
        "          last_q = last_human.content if last_human else \"\"\n",
        "\n",
        "          if last_tool is not None:\n",
        "              bridge = HumanMessage(\n",
        "                  content=(\n",
        "                      \"Use this tool output to answer the previous question.\\n\\n\"\n",
        "                      f\"Question: {last_q}\\n\"\n",
        "                      f\"Tool: {last_tool.name}\\n\"\n",
        "                      f\"Output: {last_tool.content}\\n\"\n",
        "                      \"Answer clearly and concisely.\"\n",
        "                  )\n",
        "              )\n",
        "          else:\n",
        "              bridge = HumanMessage(content=last_q or \"Please continue.\")\n",
        "          msgs = msgs + [bridge]\n",
        "\n",
        "      if self.system:\n",
        "          msgs = [SystemMessage(content=self.system)] + msgs\n",
        "\n",
        "      resp = self.model.invoke(msgs)\n",
        "      if not isinstance(resp, AIMessage):\n",
        "          resp = AIMessage(content=getattr(resp, \"content\", str(resp)))\n",
        "      return {\"messages\": [resp]}\n",
        "\n",
        "\n",
        "    def exists_action(self, state: AgentState):\n",
        "        last_human = next((m for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), None)\n",
        "        if not last_human:\n",
        "            return False\n",
        "        tool_name, tool_args = pick_tool_and_args(last_human.content)\n",
        "        return tool_name is not None\n",
        "\n",
        "    def take_action(self, state: AgentState):\n",
        "        last_human = next((m for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), None)\n",
        "        if not last_human:\n",
        "            return {\"messages\": [ToolMessage(content=\"No recent human message.\", name=\"router\")]}\n",
        "\n",
        "        tool_name, tool_args = pick_tool_and_args(last_human.content)\n",
        "        if not tool_name:\n",
        "            return {\"messages\": [ToolMessage(content=\"No tool selected.\", name=\"router\")]}\n",
        "\n",
        "        tool = self.tools[tool_name]\n",
        "        print(f\"Calling tool: {tool_name} with args: {tool_args!r}\")\n",
        "        result = tool.invoke(tool_args)\n",
        "        print(\"Back to the model!\")\n",
        "        return {\n",
        "            \"messages\": [\n",
        "                ToolMessage(\n",
        "                    tool_call_id=str(uuid4()),\n",
        "                    name=tool_name,\n",
        "                    content=str(result),\n",
        "                )\n",
        "            ]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YaFVbgfuarq"
      },
      "outputs": [],
      "source": [
        "# 5) Spin it up with HF chat model\n",
        "system_prompt = (\n",
        "    \"You are a smart research assistant. When I provide tool outputs, use them to answer clearly.\"\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=gen_pipe)\n",
        "chat = ChatHuggingFace(llm=llm)\n",
        "\n",
        "abot = Agent(chat, TOOLS, system=system_prompt, checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHxXxzQasaTT",
        "outputId": "96c86609-b616-4f2f-a409-f8549235848d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interrupted before: ('action',)\n",
            "About to call tool: search_wikipedia with args: 'Wikipedia: LangGraph'\n",
            "Change Wikipedia query (Enter to keep): apple\n",
            "Calling tool: search_wikipedia with args: 'Wikipedia: apple'\n",
            "Back to the model!\n",
            "{'messages': [ToolMessage(content=\"Page: Wikipedia:WikiProject Apple Inc.\\nSummary: \\n\\nPage: Wikipedia:WikiProject Apple Inc./Assessment\\nSummary: This is the assessment department of WikiProject Apple Inc. Its goal is to assess the quality of Wikipedia's Apple Inc. articles. Article ratings are used to help recognize excellent contributions and identifying topics in need of further work, and are also expected to play a role in the Version 1.0 Editorial Team program.\\nAssessment is done in a distributed way through parameters in the {{WikiProject Apple Inc.}} talk page banner; this causes the articles to be placed in the appropriate sub-categories of Category:Apple Inc. articles by quality and Category:Apple Inc. articles by importance.\\n\\nPage: Wikipedia:WikiProject Apple Inc./Members\\nSummary: The Members Department of WikiProject Apple Inc. is a centralized area for project recruitment and member interaction efforts.\", name='search_wikipedia', id='ad43537f-d07b-4fb0-8e42-5a0a6d27af3b', tool_call_id='a2621a73-cea6-4b51-b998-3d6629950184')]}\n",
            "{'messages': [AIMessage(content='Based on the tool output \"Page: Wikipedia:WikiProject Apple Inc./Assessment\", the assessment department of WikiProject Apple Inc. Assesses the quality of Wikipedia\\'s Apple Inc. Articles. Article ratings are used to help recognize excellent contributions and identify topics in need of further work, and are also expected to play a role in the Version 1.0 Editorial Team program. The assessment is done in a distributed way through parameters in the {{WikiProject Apple Inc.}} talk page banner, which causes the articles to be placed in the appropriate sub-categories of Category:Apple Inc. Articles by quality and Category:Apple Inc. Articles by importance. The assessment department is a centralized area for project recruitment and member interaction efforts. The tool output also mentions that the assessment is done in a distributed way through parameters in the {{WikiProject Apple Inc.}} talk page banner, which causes the articles to be placed in the appropriate sub-categories of Category:Apple Inc. Articles by quality and Category:Apple Inc. Articles by importance.', additional_kwargs={}, response_metadata={}, id='run--d0726148-e4cb-4069-a144-afe19fc33dde-0')]}\n",
            "()\n"
          ]
        }
      ],
      "source": [
        "# --- Demo: search_wikipedia ---\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "start_msg = HumanMessage(content=\"Wikipedia: LangGraph\")\n",
        "thread = {\"configurable\": {\"thread_id\": \"hitl-wiki-1\"}}\n",
        "\n",
        "for _ in abot.graph.stream({\"messages\": [start_msg]}, thread):\n",
        "    pass\n",
        "\n",
        "snap = abot.graph.get_state(thread)\n",
        "print(\"Interrupted before:\", snap.next)\n",
        "\n",
        "msgs = snap.values[\"messages\"]\n",
        "last_human = next((m for m in reversed(msgs) if isinstance(m, HumanMessage)), None)\n",
        "planned_tool, planned_args = pick_tool_and_args(last_human.content if last_human else \"\")\n",
        "print(f\"About to call tool: {planned_tool} with args: {planned_args!r}\")\n",
        "\n",
        "# ---- HUMAN OVERRIDE ----\n",
        "new_q = input(\"Change Wikipedia query (Enter to keep): \").strip()\n",
        "final_q = new_q or (planned_args if isinstance(planned_args, str) else \"LangGraph\")\n",
        "\n",
        "# Instead of resuming the queued action, start a fresh run with the edited query\n",
        "thread2 = {\"configurable\": {\"thread_id\": \"hitl-wiki-2\"}}\n",
        "for _ in abot.graph.stream({\"messages\": [HumanMessage(content=f\"Wikipedia: {final_q}\") ]}, thread2):\n",
        "    pass  # pause before action again\n",
        "\n",
        "for event in abot.graph.stream(None, thread2):\n",
        "    for v in event.values():\n",
        "        print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phXPR-Ym1chF"
      },
      "source": [
        "# Bonus: Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "TAGIj5-D1WPY",
        "outputId": "d4ba81f1-5837-4d72-e10a-88df198babf9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Image, Markdown\n",
        "\n",
        "g = abot.graph.get_graph()\n",
        "\n",
        "try:\n",
        "    png_bytes = g.draw_mermaid_png()\n",
        "    display(Image(png_bytes))\n",
        "    with open(\"/content/agent_graph.png\", \"wb\") as f:\n",
        "        f.write(png_bytes)\n",
        "except Exception as e1:\n",
        "    try:\n",
        "        png_bytes = g.draw_png()\n",
        "        display(Image(png_bytes))\n",
        "        with open(\"/content/agent_graph.png\", \"wb\") as f:\n",
        "            f.write(png_bytes)\n",
        "    except Exception as e2:\n",
        "        mermaid_src = g.draw_mermaid()\n",
        "        display(Markdown(f\"```mermaid\\n{mermaid_src}\\n```\"))\n",
        "        print(\"Rendered Mermaid source (install Mermaid or Graphviz to get an image).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3DEJn-Qax-m"
      },
      "source": [
        "## References\n",
        "1. [LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph) â€“ curated courses & tutorials:    \n",
        "  GitHub: <https://github.com/langchain-ai/langchain-academy.git>\n",
        "\n",
        "2. [LangGraph Cookbook](https://github.com/abhishekmaroon5/langgraph-cookbook)\n",
        "\n",
        "3. [LangChain Tutorials](https://python.langchain.com/docs/tutorials/)\n",
        "\n",
        "4. [Interrupt](https://interrupt.langchain.com/) â€“ the AI Agent Conference by LangChain  \n",
        "\n",
        "5. [A Hands-On Guide to Building Intelligent Systems](https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/preview?tab=t.0), Antonio Gulli\n",
        "\n",
        "<!-- 6. [Prof. Ghassemi Lectures and Tutorials](https://www.youtube.com/@ghassemi), AI Agents lectures -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hvuC2odr0tV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
